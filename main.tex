% LRP2020 white paper template
% Search for "Instructions" below, and also see the call for white papers
% https://docs.google.com/document/d/1IT0g5AqaQM2FQQ0--M9qyQuWQ2906WlK_R-O32ZYoSY/
% Please don't change the page headings, margins or font size.
% HISTORY:
% 2019/06/27: v1.0 original version, v1.0
% 2019/07/12: v1.1 instructions added in executive summary section, re: cover page. 
% Changes to wording of text box questions 2, 6, 7. 
\documentclass[11pt]{article}
\usepackage{times}
\usepackage{geometry}
\geometry{letterpaper, portrait, margin=2cm}
\usepackage[utf8]{inputenc}
\usepackage{enumitem,amssymb}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{aas_macros}
\usepackage{mdframed} 
\usepackage{url}
\usepackage{hyperref}

\usepackage[authoryear]{natbib}
\bibliographystyle{apj}
\setcitestyle{authoryear,open={(},close={)}}

\mdfdefinestyle{theoremstyle}{
innertopmargin=\topskip,}
\mdtheorem[style=theoremstyle]{lrptextbox}{}

\pagestyle{fancy}
%Instructions:
%Please insert your expression of interest number of the form ENNN; see https://docs.google.com/spreadsheets/d/1_GqBxICZL0di_KvQoi_ZrdfNvqNnGotSYAoq0UqJYGc/ 
% and title (shorten if necessary) in the line below
\rhead{DRI-Astronomy}
\lhead{\thepage}
\renewcommand{\headrulewidth}{0pt}
\cfoot{}

% ****BEGIN EXECUTIVE SUMMARY SECTION****
% Instructions: A 5000-character-or-less executive summary will be requested on the white paper submission form.
%
% The white paper submission form will generate a cover page that will include the executive summary, topic area, author list and lead author contact information. Please do not include this information in the PDF generated with this template.


\begin{document}
% ****BEGIN MAIN WHITE PAPER SECTION****

% Instructions: Please insert your white paper text here.
%A white paper should be a self-contained description of a future opportunity for Canadian astronomy. A white paper will be most effective and useful if it concisely summarises and recommends an option that the LRP2020 panel should be considering for prioritisation.
%
% White papers are not required to contain a specific set of sections or headings. Depending on the content, the following topics may be appropriate to include:
%connection or relevance to Canada
%timeline 
%cost 
%description of risk
%governance / membership structure 
%justification for private submission of supplementary information
%This white paper will examine the expected evolution of digital research in support of astronomy research, focusing on the opportunity to create a digital research science platform for Canadian astronomy.
\setlength{\bibsep}{0.0pt}
\author{}
\title{Digital Research Infrastructure in Astronomy}
\maketitle
\section{Summary}
In this White Paper we use the term Digital Research Infrastructure (DRI) to refer to the hardware (e.g. compute, storage, networks) and software (e.g. OS, middle ware, science platform) layers and to the support of users on those systems. 
The Canadian astronomy community requires reliable and stable access to Digital Research Infrastructure (DRI) that is significantly resourced and presented via interfaces designed to serve the science user.
The current funding model for both the hardware and software layers of Canada's DRI is evolving and astronomers must take action to ensure that the new organization that forms to manage DRI funding (currently referred to as `newORG' and `engageDRI') is aware of and able to address the community's needs.

{\bf We propose a pathway that will lead towards the creation of the Canadian Astronomy Science Centre (CASC; working title) as a partnership between National Research Council (NRC) and the Canadian astronomy research community.  CASC will become the entity responsible for management and advocacy of DRI in Canadian astronomy. }

Astronomy must have a coherent national voice through CASCA, but incorporating the expertise of CADC, CANFAR, and university science groups is important for Canada to maintain its leadership in DRI.

{\bf Recommendation 1:} Form a CASCA committee to develop strategies for the effective utilization of NRC, CANARIE, CFI, Compute Canada, and other DRI funding sources for the benefit of Canadian astronomy. This committee should develop recommendations and report these to the CASCA Executive who can then engage with all of the involved funding partners to ensure a coherent model for DRI in astronomy.

Canadian astronomy needs to increase the funding going into DRI and we must develop new community-wide structures to manage this infrastructure. 
The CADC, backed by continuous NRC funding, is a central element of this infrastructure due to its expertise in building and maintaining stable, world-leading DRI that is has developed over the last 30 years. 
Currently, funding of the hardware and software platform for research is being maintained via project focused-CFI grants (such as CIRADA, CLASP, CHIME, etc.) within a funding model that excludes direct support for the CADC.  
Separately, NRC has continued to support CADC's efforts to build the CANFAR science platform capacity so that this can act as unifying system for the various individual projects.
Maintaining the CADC's world-leading expertise in the current system of funding via CFI grants whose funding distribution excludes CADC involvement is highly challenging. 
Particularly challenging, from a data volume and computing needs view, will be supporting SKA, CHIME and possibly LSST data within the increasingly complex data landscape. 
Ongoing and increased NRC funding to CADC is needed to deal with the challenges of creating an astronomy cyber platform that integrates Canada's astronomy data sets. 

{\bf Recommendation 2:} CASCA should communicate the increasing importance of a science platform for astronomy to NRC and encourage increased support to CADC to ensure that a robust platform is developed. In utilizing this increased support, CADC should develop a budget that is examined, critiqued, modified, and endorsed by a sub-committee of CASCA in communication with NRC.

Where university-based groups are able to create and manage some components of the necessary science infrastructure, they should be encouraged to do so. The principal challenges for university groups are 1) to create infrastructure that serves a broad science community rather than a focused research project, 2) to integrate the infrastructure they create with existing national and international astronomy infrastructure and, most importantly, 3) to ensure the continued operation and ongoing development of their infrastructure reasonably far (more than a decade) into the future.

{\bf Recommendation 3:} CASCA should communicate with those other bodies that provide funding and resources for astronomy and demonstrate a unified vision of the importance of digital infrastructure to astronomy. The complementary strengths of NRC/CADC and university groups should be emphasized.

{\bf Recommendation 4:} The dialogue and processes established in Recommendations 1,2,3 leads to the creation of a formal entity (Canadian Astronomy Science Centre) to provide a bridging partnership between the various funders and actors in Canadian astronomy DRI and establish the capacity and breadth of mission needed to create and maintain a robust Digital Research Infrastructure in Astronomy.

\section{Introduction}

As can be seen from even a cursory examination of the other LRP2020 white papers, Canadian astronomy is a data-rich research endeavour.  Astronomical research has evolved to become a digital science, dependent on methods of analysis (W010, W004) digital infrastructure (W011, W015, W026, W046, CDC submission) and the collection of large survey data sets (W006, W018, W020, W025, W030, etc.).  The data-related theme that runs through these white papers is that Canada has benefited strategically from the creation of the Canadian Astronomy Data Centre (CADC) and the community's ambitions reach well beyond the capacity of the current facility. 

To tackle fundamental scientific questions, astronomers are turning to ever larger data sets, made from surveying the sky with a variety of observational facilities operating across the energy spectrum. We are building up a picture of the universe over a broad range of resolutions, timings and messengers.  At the same time, computing and mathematical methods are evolving towards techniques that are driven by AstroInformatics, Statistical Learning or Machine Learning (such as stellar classification and redshift determination) and even evolving towards systems that use artificial intelligence in their analysis.  Within astronomy, we must pick up the pace in deploying these new technologies and in training the workforce.


\subsection{The diversity of DRI}
When considering digital research infrastructure solutions in astronomy one must keep in mind the diverse scope of the discipline of Astronomy and Astrophysics. 
The community studies physical processes that span scales from the sub-atomic to, literally, the size of the universe.  
The various fields study processes that are driven by Newtonian physics, General Relativity, Fluid dynamics, weak and strong forces, and chemical and mechanical actions. 
This broad array of physical scales and mathematical approaches requires support from a diverse cyber-infrastructure.
While some problems are well addressed by massively parallel {\bf supercomputer} systems using high-speed interconnected processors, other problems are better addressed through the use of dedicated GPU processing on a set of {\bf specialized hardware}.  
Still other problems are better considered as {\bf high throughput computing} where massive amounts of data are processed in parallel via machines with some limited interconnected capacity and access to high-speed storage systems.  
Other problems are better suited to systems that straddle interactive and high-throughput computing making use of {\em virtualization and cloud infrastructure}.
When considering the solutions for cyber-infrastructure in astronomy it is important to keep in mind that this broad range of scales and capacities must all be satisfied for our diverse field to flourish and maintain its world-leading impact. This white paper adopts the following definitions:

\begin{itemize}
    \item[Compute:] This includes the capacity of computing needed to process raw data from a telescope into a science-ready data product as well as the capacity needed to then turn that science product into scientific insight.  Systems include the super-computing capacity needed to correlate radio signals from multi-antenna arrays\footnote{This White Paper focuses on observational astronomy use of DRI but the science platform approach that we advocate should also enable bringing numerical simulation results into an environment where they can be assessed against observations, where appropriate.}, high-throughput computing needed to combine data from multiple observing sessions, specialized GPU and similar systems needed to develop and train complex models (increasingly based on Machine Learning concepts) and the cloud infrastructure (including the high-speed data layer) needed to allow direct, responsive interaction with large data sets.
    \item[Networks:] In order to process the data from the array of facilities to which Canadians have access, we must have the capacity to transfer those observations over the research internet.  Within the next decade the Vera Rubin Survey Telescope (LSST) will generate 2-3 petabytes of exportable data annually, requiring sustained bandwidth of 1 Gbit/s just to transfer a single copy as a continuous stream over one year and the Square Kilometre Array will generated multiple petabytes of data annually requiring networks capacity 10 to 100 times larger.  Big data projects, such as CHIME in particular, have been limited by the lack a robustly interconnected (CANARIE+NRC) research network.
    \item[Storage:] The CADC currently houses approximately 2 PB of astronomical observations, representing over 3 decades of archiving activities.  The LSST will produce this volume of data annually!  The CADC is currently expanding its capacity to achieve 5 PB of usable storage but this growth will only satisfy a few years of the data rates expected from facilities that are currently being archived (CFHT, JCMT, Gemini, ALMACA, TAOS-II, HST, JWST, DRAO, DAO).  The existing infrastructure cannot support the next decade's data behemoths.   Nationally, across all research disciplines, Compute Canada's (CC's) system provides aggregated project storage of about 24 PB. This volume is about the same size as would be required for the LSST-lite data centre alone (W015).
    \item[Databases:] Here we refer to not the observational data files that are produced but to the catalogs of information derived from those observations.  Observational astrophysics, like most disciplines, is evolving into ever more specialized perspectives on problems. Roles in the observational problem often break-down into segments along a spectrum.  This spectrum includes the engineering and physics challenges of building ever more sensitive facilities to collect the signal (LIGO, IceCube, SNO, LHC, ALMA, Gemini, CFHT, JCMT, etc.) and their cohort of instruments and detectors that digitize that collected signal.  The other end of the spectrum includes the software systems that analyze that digitized signal to produce catalogs of measurements, which can then be analyzed and correlated with signals from other facilities.  Each facility can receive different messengers of the same event. These catalogs from different facilities are being stored in ever larger databases of information. 
    
    In the era of LSST and SKA, the databases of measurements will grow in importance for science exploitation since the handling of the detector outputs will become a more specialized activity.
    As of this writing (Fall 2019) the CDS-SIMBAD database in Strasbourg (which strives to aggregate the published astronomy information available for all identified objects and is updated on a daily basis) contains just over 35 million measurements of 10 million different objects.  The CDS-Vizier system distributes tabular information accumulated in the literature and contains nearly 20,000 tables of data.  However, new catalogs like Gaia contain billions of entries.  Gaia, the coming LSST science catalogs, then SKA catalogs are produced as part of the facility operations, and the science end user will achieve their science goals through the direct interaction of `observing' the catalog data.  These catalogs will be multiple petabytes in size.  For even more sophisticated analysis the `observer' will combine together information gleaned from examination of the cataloged measurements with access back to the detector outputs (spectra, pixels, time-series voltages), likely in processes driven by increasingly sophisticated machine learning approaches.  {\bf Canadian Astronomy has developed strong expertise in compute and storage components and has capacity in network infrastructure but is lacking in database and catalog exploitation capacity. } 
 
    \item[Software:] For the purposes of this white paper we consider three broad categories: science analysis software, client application software for accessing infrastructure, digital services that expose the base infrastructure to the client.  Most instrumentation facilities have developed some level for each of these broad components, but few (if any) have  complete systems.  One of the great risks facing astronomy, and Canada in particular, is that we will not have the software systems needed to fully exploit the data sets we are creating.  At the infrastructure level the NRC, CANARIE and the Canadian Space Agency (CSA), through the operation of the CADC, the International Virtual Observatory Alliance (IVOA) and the CANFAR science platform, are providing a strong base from which to base to build.  Recently the CFI funded Canadian Initiative for Radio Astronomy Data Analysis (CIRADA) has begun to attempt to fill the gap in the science analysis component, but this gap is very large.  In particular the CIRADA project, by its very nature, is focused on the radio-specific pieces of the problem.  In addition, the focus of the efforts of CIRADA is on transforming calibrated observations into science-ready data products, leaving the final hurdle of transforming those data products into science an unfunded activity.  The Canadian LSST Alert Science Platform (CLASP), a CFI proposal at this stage, is attempting to mirror the CIRADA project but for the LSST optical community.  What will become of the expertise developed through CIRADA and possibly CLASP when the projects end is unknown. {\bf A coherent centre for software systems must be advanced to address the broad range of software needs within the observational community.}
\end{itemize}

In the petabyte era the lines between software, technology and science are blurred - the likelihood to doing science with petabytes-scale datasets without major infrastructure designed and operated to meet the need of the science user will be very low.  
%The importance of technology in science exploitation is becoming ever more important.
In a modern astronomical research community DRI should present to the community a domain-specific view of the resources, providing access to software systems and tools that are designed to meet the research goals of that community.  Such a `Science Portal' should be seen as facility in the same way that a telescope is a facility and the components of the systems are developed and replaced via instrumentation programs.  This is needed to help ensure that the astronomy community is getting highly effective access to DRI resources provided at the national level.



\section{The DRI Landscape}

\subsection{CADC}

Formed in 1986 (CADC will have its 40th anniversary during the implementation of LRP~2020), the CADC was envisioned to act as a centralized facility to provide access to both technical expertise in applications as well as physical capacity in data handling and processing. 
The founding data collection was seen as that coming from the Hubble Space Telescope, but from the start the facility was expected to bring together data sets from across the observational spectrum to enable world-leading astronomical research. 
Although located in a decentralized location (Victoria, BC), then-emerging network technologies made it clear that geography should not be a barrier to access.  However, the path between 1986 and today has not always been smooth.

%Discussion of creation of the CADC dates from early 1980s with the official formation occurring around 1986.  The Winter Solstice 1986 edition of Cassiopeia contains  Newsletter \#1 from the new Canadian Space Astronomy Data Centre, then led by Andy Woodsworth.  
The initial mission of the Canadian {\it Space} Astronomy Data Centre (CSADC) was to:
\begin{itemize}
\item To obtain, catalog, and archive copies of all public domain data from HST (most data from the HST become public domain one year after observation; the remainder become public domain immediately).
\item To make copies of requested portions of such data available to all Canadian scientists.
\item To operate a data reduction facility for the benefit of all Canadian astronomers.
\item To obtain and develop software suitable for the interpretation and reduction of HST data.
\item  To assist and advise Canadian scientists in setting up their own data reduction facilities where appropriate.
\end{itemize}

The initiators of the CSADC recognized that the data volumes and computing requires from HST would be complex for the majority of the community to properly handle.  This was in 1986, just as NRC was experiencing substantial funding cuts and HIA was being forced to take drastic steps to reduce their budget.  However, strong support from the astronomy community was found and CSADC was quickly changed to become the CADC.  While the community awaited the launch of HST the CADC took action to take advantage of the opportunities available:

\begin{quote}
    {\it Since the proposal was developed, we have seen the potential for related services, such as providing access to a number of other astronomical catalogs and databases (e.g. IRAS, IUE) and facilities for analysis of large-format CCD images. We plan to offer these and other services as well as those strictly related to HST observations \citep{1987CassCSADC}.}
\end{quote}

Even from its inception, however, CSADC struggled to achieve the budget capacity needed to achieve its mandated goals.  After significant discussion, the Canadian Space Agency (CSA) entered into an MOU with NRC to co-fund the activities of the CADC. Under this MOU, CSA provided, at the time, roughly 50\% of the support required to operate the CADC.   

%Initially, CADC made technology choices that have proved highly successful and enabled the organization to be centre for astronomy archive innovation. This initial choice was to not mirror the systems and hardware operated by STScI for HST but instead to migrate information from that system to CADC's own systems. This allowed the CADC to pursue innovative search and retrieval options and was critical to the eventual development of 'on-the-fly' recalibration of HST data products and the production of deep stacks of HST imaging.  Importantly, at that point the CADC infrastructure used to support HST could also be used to support CFHT archiving which, initially, was the primary activity due to delays in HST launch and then problems with early operations. 

The insightful original mission of the CADC is even more relevant today.  Although there have been challenges, including NRC's consideration of eliminating the CADC entirely, HAA-NRC continues to support the important mission of the CADC in partnership with the CSA. 
DRI for research is considerably more evolved today than in 1986 but the fundamental need for astronomy research to have a domain-aware access layer remains. 

\subsection{CANFAR}
Starting in the late 2000s, the CADC partnered with university researchers to form the Canadian Advanced Network For Astronomy Research (CANFAR) to develop a science platform for astronomy layered atop Compute Canada's grid infrastructure.  
This project was funded under CANARIE's Network Enabled Platforms program.  
Since its first implementation, the CANFAR platform has undergone three technology transforms, each causing substantial modification of the computing and storage interfaces.  
These modifications have required substantial effort to achieve.  Although it has existed for nearly a decade, CANFAR still requires substantial evolution to become a complete astronomy science platform. 

\subsection{DRI Funding in Canada}

The funding landscape for DRI consists of a complex, overlaping and sometimes conflicting set of programs. 
At the time of writing, computing and storage infrastructure in Canada is operated by Compute Canada Inc.\ (CC) via support provided by federal and provincial governments but is in a state of substantial evolution.  
For a discussion of this history and the coming changes please see the Computing and Data Committee submission to the LRP2020 panel. 
In brief, during the spring of 2019 the federal government of Canada initiated a process to restructure how computing is provided for research through the establishment of a new entity that will operate the national computing and storage systems in addition to supporting research software.  
The operation of research networking will remain with CANARIE.  
CANARIE has also been the funding agency for `science platforms` and for developing research software and the community of research software engineers.
CANARIE's non-network roles are being transferred into the newORG. The details of what will be included within the newORG's funding model will be critical to astronomy.
The Canadian Foundation for Innovation (CFI) has supported the development of DRI via their  Cyberinfrastructure Initiative:
\begin{quote}
    We support research data infrastructure projects that create tailored, shared and integrated data resources (e.g. databases and data repositories) capable of enabling leading-edge research on significant scientific, social and economic questions \citep{ciwebsite}.
\end{quote}
In addition, university researchers develop software and purchase mid-scale computing infrastructure using NSERC Discovery Grant and Research Technology Infrastructure grants.
Meanwhile NRC in partnership with the CSA has provided long-term and often substantial support to CADC to provide long-term data preservation (archiving) and to support the operation of an astronomy science platform on-top of CC provisioned infrastructure that has been awarded to CANFAR via CC's Research Platforms and Portals competitions.  Although CADC is eligible to receive, via university projects, CANARIE funding via fee-for-service work that develops the CANFAR science platform, CADC is not eligible to receive support from CFI or NSERC for those same activities. 

It is still early days in the establishment of newORG and the community must pay attention to ensure that the details of the evolution meet our requirements. 
We must work to ensure that the support and funding models that are adopted will be effective for the Canadian astronomy, in particular when considering the partnership between NRC and university support observational facilities, CADC, CANFAR and the university research communities cyber-infrastructure requirements.
The turmoil in the evolution of computing has been going on since the last CASCA LRP, but there is some optimism that the new organization will be better funded (based on pledges made by the previous federal government) and run in a more research-focused way (based on the draft structure that the new operator is expected to adopt). 

%\cite{2006JRASC.100....3S} Astronomy and Astrophysics are a data-rich research fields.   The world possesses a dizzying array of observational facilities (Alma, VLA, Subaru, Gemini, Apache Point Observatory, KPNO-4m, Hale, VLT1,2,3,4, SOAR, Magellan, Greenbank, DRAO, DAO, etc. etc.) and increasingly complex, more complete and data rich models (e.g. The Millennium Run, NuGrid, \textcolor{red}{OTHERS}), in addition to the existing (e.g. Gaia, SDSS, PanStarrs) and planned all-sky surveys (LSST, Euclid, WFIRST).   


\subsection{Science Platforms}
Canadian astronomy requires digital research infrastructure that can bring together these various digital pieces:  computing, storage,  networks, databases and software.  {\bf Given the expressed science need for data collections to cut across sub-domains (such as X-ray, optical, optical, infrared and radio astronomy) a single astronomy domain aware science portal that enables use of the full spectrum of this data is needed.} 
Such a system must be built in an agile way and respond on short timescales to the changing requirements of emerging science activities and evolving technologies.  In addition to CANFAR, a few science platforms already exist internationally (notably  \href{http://sciserver.org}{JHU's SciServer}, \href{https://datalab.noao.edu/}{NOAO DataLab} and \href{http://astrocloud.china-vo.org}{ChinaVO}) and many more are being planned or built, (e.g. \href{https://projectescape.eu/services/escape-science-platform}{Project Escape} and \href{https://docushare.lsst.org/docushare/dsweb/Get/LSE-319}{LSST Science Portal}).  Through the IVOA, the astronomy community is working towards standardization of these science platforms.  Canadian astronomy must include a comprehensive science platform as part of our DRI.

As the primary host of the Canadian Virtual Observatory, the CADC has been participating in the IVOA efforts from their outset to ensure that data centres provide standardized interfaces that enable interoperability.  
The success of this effort can be seen, for example, in the rich universe of TAP services that allow astronomers to probe an incredibly diverse sets of astronomically relevant catalogs of information. 
These efforts lead to what might be considered `open data' in astronomical research. 
As we move into the Science Platform realm this concept of standardized service layers is even more critical as they provide the opportunity for platform interoperability, permitting scientists to have their research investigations to span between centres.  
As we work towards the concepts of `bringing the code to the data,' which is a fundamental driver of the science platform concept, we must keep in mind the interoperability of these science platforms or we will lose the achievement of open data by closing off the resources needed to access those data.

At this time, CANFAR has more computational resources than most of the existing astronomy science platforms (all except fro ChinaVO which, as of Fall 2019, has over 2000 active research users 10s of PB of storage and 10,000s of cores). 
However, the user interface to CANFAR requires substantial development to allow the same ease of use that is being achieved (or planned) elsewhere.
These science portals are executing a vision of bringing astronomy research computing into a cloud-based environment that crosses boundaries across wavelength and research domains. 
The {\it SciServe portal} reaches beyond the astronomy community, providing similar capabilities across the sciences and humanities research groups at Johns Hopkins University.
The concept of a science portal as the gateway to computing has become a ubiquitous modality. 

To be useful, these portals require stable long-term funding on top of the base infrastructure funding. Without this stability, researchers will become reliant on systems that perpetually shift in their behaviour, and this will substantially impact productivity.  

One example of the risks posed by the absence of good software support is the TOPCAT program.  
TOPCAT is a core component of many astronomers' workflows, which allows exploration and analysis of tabular data.  
This power tool allows users to access 1000s of astronomical databases from a single application and cross-matches these catalogues for science analysis but is constantly at risk of being lost due to a poorly matched funding model.
TOPCAT was initially developed at the University of Bristol under the Starlink project
(2003 to Starlink's demise in 2005).
In 2005 TOPCAT support and development was picked up by a combination of AstroGrid (the UK's VO project) and various related Euro-VO funding streams (central EU funding) until 2011, by which time TOPCAT had become embedded within the IVOA,
The Table Access Protocol (TAP) is an IVOA standard for over-the-wire access to databases. 
TAP was developed by the IVOA to standardize access to databases over http(s).  
As TAP evolved (it is now nearly ubiquitous in astronomy data access) there was no capacity to support integration of new TAP components into TOPCAT and the utility of the underlying TOPCAT program was at risk of being lost if it could not be adapted to integrate these TAP services.
In 2011 funding for AstroGrid was discontinued just as TAP was achieving wide implementation and the TOPCAT project was nearly discontinued. 
However, the German Astrophysical Virtual Observatory (GAVO) provided some bridge funding (6-months), specifically to add the TAP functionality to TOPCAT.
Since the end of that `bridge funding', TOPCAT has survived via
grants from the UK-STFC's Astronomy Grants Panel and 
some Gaia space mission-specific funding.
The Gaia funding was achieved as TOPCAT is now recognised as a primary way that astronomers work with large catalogue data.
During this entire process the core developer of TOPCAT has remained stead-fast, sometimes working without pay, in an attempt to provide the research community with a fundamental tool for its success. 
This example illustrates the precarious survival of a fundamental tool for astronomy research caused entirely by constantly shifting funding processes that fail to recognized the requirement of stability for software projects to survive.

A similar case study comes from The University of Postdam, which provides access to many datasets via web interfaces that they have created (using their in-house  `daiquiri' framework).  Recent funding changes at Postdam mean that the lead developer of `daiquiri' is now `self-employed' by individual projects that need parts of the system updated and the main servers at Postdam are kept operational by a single person, who may soon move to other research activities.  

These projects are examples of high-value activities in the international astronomy community that are being run out of institutions that are not in a position to provide the long-term support that such services must have.  
Contrast these with services like the `Canadian Galactic Plane Survey' or the Herschel Archive which continue to operate and deliver high-value products after the end of mission.  
Those are examples of services that are being provided by institutions with a mandate to maintain their long-term viability. 
{\bf Long-term stable funding is key for successful DRI.}

\section{Needed infrastructure}
The astronomy research community, along with other disciplines,  has a growing and strong need for increased storage and compute capacity accessed via domain specific science platforms.
Examining the various white papers that have been submitted for LRP2020 we can see that over the next decade storage capacity must grow from the current scale of a few petabytes of online storage (enabling the storage of all Canadian astronomy data on live disks) to the capacity of 30-100s of petabytes of online storage and many times more in near-line capacity (i.e., tape). 
This capacity is at the scale of one of the entirety of one of CC's current general compute centres (Cedar, Arbutus, Graham, BÃ©luga and Niagra). 
The CHIME project, for example, already generates petabytes per year of operation while LSST will generate tens of PB and SKA will generate hundreds of PB.  
And astronomy is not alone:  high energy physics, genomics and climate (for example) are also demanding increased capacity for large data.  
We must, at this time of transition for Canada DRI management, clearly voice the needs of the research community.
{\bf The existing capacity within the CC's centres will be exhausted on very short timescales.}

Along with the predicted increase in storage data volumes will come a need to increase our computing capacity.  
Within CANFAR, for example, the majority of computing is to reprocess observational datasets in novel ways to extract new information.  
Thus, if we increase the stored data by a factor of 10--30$\times$ we must also anticipate a need for increasing our compute capacity by a similar factor.  
The need for increased computational capacity is driven higher still by the development of new approaches to data analysis, in particular convolutional neural network (CNN)-based machine learning.  
The CNN approach invariably requires access to GPU-based computing to make possible the numerical computation needed to train deep networks. 
During the most recent round of CC computing allocations, the demand for GPUs significantly out-stripped the available capacity by an oversubscription rate that is even larger than for standard CPU processing. 
With growing data volumes and increased efficacy of techniques, the pressure on classical and GPU computing will only continue to grow.  
Here too, the data-intensive research communities must organize themselves so that the appropriate models for delivering computing infrastructure can be developed.

Given the volumes of data, tremendous computing requirements and the desire to provide access to that infrastructure via software systems that can expose the resources in ways that enhance the use of domain knowledge, {\bf we propose the formation of the {\em Canadian Astronomy Science Centre} as a University-NRC partnership that will build, operate and maintain the extensive DRI that is needed for the future of research astronomy}.  CASC will bring together the existing infrastructure pieces (CC or newORG, CADC/NRC, CANFAR etc.) and funders (NRC, CFI, CANARIE, newORG) to establish a national astronomy science platform to meet the needs of our current and planned observational facilities. 

\section{Connection and relevance to Canada}
The CADC and CANFAR  provide the Canadian astronomy community with a strong advantage.
Canadian astronomy has a historic leadership role in the development of data standards relevant to astronomy via our participation in the IVOA.  
We also have a long history of providing archive services for astronomical facilities and of connecting that archive storage to processing.  CADC staff span a range of activities from operations to development through to science use and public dissemination.  
The historic background and staffing design has made the CADC facility an important asset for the Canadian community, one that can, and has been, leveraged to provide access to other resources and facilities. 
For example, CADC expertise and provision of the JCMT archive has allowed Canadian scientists to continue using the telescope after the formal participation in the telescope concluded.

In the coming decade Canadian astronomy has ambitions to participate in a growing array of data rich experiments:  Euclid, LSST, SKA, and WFIRST.  Participating in these projects will require the capacity for storage and processing to grow from the current level of 2-3 PB of active storage to 50-100 PB of active capacity.  There does not exist, today, capacity within Compute Canada's community storage system sufficient capacity to enable the astronomy's ambitions. Along with this storage will come a need for processing.  These computational needs will roughly scale with the annual growth in storage.   In the current installation, 5 PB of data  requires approximately 1500 CPU cores of processing for effective scientific use, so we estimate that a 50-PB storage system will require 15,000 CPU cores.  

In addition, software systems capable of managing the storage and processing system must be developed and enhanced.  Within the SKA project there are plans to adapt CERN-LHC storage systems like EOS and RUCIO for use in astronomy.  These software systems have a strong pedigree but have been designed to satisfy the particle physics communities needs and modes of operations.  Astronomy will need to invest significant effort to ensure software technologies exists to meet our community's needs.  These technologies should support IVOA-developed standards and it is conceivable that the storage systems themselves could emerge as a community standard. 

As Canada seeks to grow its digital economy, astronomy data storage and processing needs could be used as a catalyst for enhancing our national digital capacity.  
CASC would become the driving organization for this new era of digital research in astronomy. 

\section{Timeline}
The major data missions will require storage and processing capacity to rapidly grow, beginning in the early 2020s.   By the end of the next decade, Canadian astronomy will require a fully operational storage and processing capacity capable of handling $\ge 50$ PB of astronomical data. This is about 25 times the capacity that is currently (Fall 2019) being deployed for astronomy research needs.  We must act quickly to be able to meet the needs of the community and remain world-leading researchers in astronomy.

\section{Cost}
Storage and processing costs have been trending lower and are likely to continue to do so as technology enables higher density of capacity.   As an example of the projected costs for data centres, the Canadian SKA Regional Centre Advisory Committee established the best-available cost estimates for running a science and data centre for the SKA. They estimate the cost of running a 23 PB storage system at roughly \$4.5M annually, including all staffing (including some science user support), storage and processing costs, giving a full cost of \$200K/PB/year for storage and all its concomitant processing needs. These costs are priced in 2019 CAD, and we expect that this number would scale lower in the 2030s and beyond.  Since it is is likely that the community's storage needs would simultaneously grow,  it is likely that the annual budget to support CASC will be somewhat flat.

\section{Description of risk}
There is not capacity with the current CC general storage and computing system to support the desired level of storage and processing.  In addition, the CC operations model does not fully support the astronomy research community's needs (the same is also true of other data-rich research communities, such as high energy physics).   As the new organizational structure for DRI is developed, this organization may make things better or worse. Canadian astronomers must strongly engage with this new organization to ensure that the our needs will be properly addressed.  CASC would create a stronger voice for astronomy within this conversation.

In addition to the hardware systems and their architecture, the community also requires a science platform to make use of those hardware systems transparent and efficient.   This will require building a strong astronomy-specific science platform.  The world astronomy community is already engaging in the construction of such systems and Canada must continue to strongly engage in this effort.  In the current funding model, university groups are funded to develop specific pieces relevant to their research needs.  However, we do not want that effort to be siloed within the specific research problem and those research infrastructure pieces must be supported beyond the life of the project that created them, or the effort will be wasted.  The Canadian Astronomy Science Centre would provide an umbrela underwhich to unite these various pieces.

\section{Governance/membership structure}

\begin{lrptextbox}[How does the proposed initiative result in fundamental or transformational advances in our understanding of the Universe?]
Individual telescope and research projects currently must build their own access layer to enable astronomy research on Compute Canada Federation (CCF) infrastructure.  The CASC will remove this burden from the individual projects and provide enhanced capacity to users, reducing the time lag between data acquisition and science (i.e., increase productivity).
\end{lrptextbox}

\begin{lrptextbox}[What are the main scientific risks and how will they be mitigated?]
The main risk is that the science platform infrastructure will not meet the needs of the community it is meant to serve.  This risk will be mitigated by establishing a governance and advisory structure that gives the stakeholder community control of the resource development.  This body will run a Science Advisory group that will review the development choices of the platform to ensure they are working towards meeting the broad goals of the community.  
\end{lrptextbox}

\begin{lrptextbox}[Is there the expectation of and capacity for Canadian scientific, technical or strategic leadership?] 
The Canadian Astronomy Data Centre has developed strong expertise in the field of  astronomy data management. This expertise is evidenced by CADC's leadership role of the development of data standards within the International Virtual Observatory.  The CANFAR science portal as it stands today provides some of the service capabilities that are needs by an astronomy science platform and has been developed by the CADC in collaboration with CCF and the university research community. 
The University research community has significant leadership in developing specific infrastructure, such as CHIME, CIRADA, BLAST, etc.  CASC will bring these leadership strengths together in a strategic collaboration to advance the very real needs of the community.
\end{lrptextbox}

\begin{lrptextbox}[Is there support from, involvement from, and coordination within the relevant Canadian community and more broadly?] 
CANFAR has some of the required structure to allow engagement with the national community.  
Recently, however, a number of  specific and separate portals within the astronomy community have been or are being developed. 
CANFAR, through the CADC has been working with the specific projects to ensure these efforts will be integrated into the astronomical DRI initiatives and maintained over the long term.
During the Wide-Field astronomy town hall meetings there was general concensus that Canada can only support a single DRI platform.
CASC would help to cement the concept of a national comprehensive science platform for DRI in astronomy. 
\end{lrptextbox}


\begin{lrptextbox}[Will this program position Canadian astronomy for future opportunities and returns in 2020-2030 or beyond 2030?] 
Building a science platform along with the storage and computing capacity required for the various projects planned in the 2020s will help build the capacity required for Canadian astronomers to continue to be participants in modern astronomy.  The big-data projects like LSST, SKA and Euclid will begin operation in the 2020s and continue will into the 2030s.

\end{lrptextbox}

\begin{lrptextbox}[In what ways is the cost-benefit ratio, including existing investments and future operating costs, favourable?] 

We consider the cost-benefit ratio for three separate categories of DRI:  archival/team storage/compute, project/facility storage/compute, and software systems.

{\bf Archival/team storage and compute} are cheap and the benefits of these systems are substantial.  The cost of computing and storage roughly scale together.   Averaged over expected pricing between 2020 and 2030 the full costs of operations is about \$200K/PB/Year (assuming one is running at 20 PB storage facility,  development and support staffing and some other costs do not scale directly with storage/compute levels). The cost of 1 PB of storage and associated support is comparable to the cost of single full-time staff position at a Canadian research university.  

The current CADC/CANFAR storage use a proxy for this style of storage. In 2018, 4606 individual computers and 248 authenticated users (most data transfers are for public data and are anonymous) connected to the CANFAR/CADC storage system and retrieved about 1 PB of data from that system.  The CADC/CANFAR  storage system currently has a capacity of about 2 PB.  The hardware costs of supporting those 248 users has a cost similar to that of funding a research faculty position.  

{\bf Project/Facility storage and compute} costs should be considered as part of the operational costs of the project or facility at the time of funding. Projects such as LSST, CHIME, and SKA must be evaluated on the full life-cycle costs of those projects.  However, Canada benefits greatly by having a base level of institutional capacity in this area so that the incremental costs to these specific projects can be minimized.

Expanding the storage and computing capacity of the systems developed and maintained by the CADC within CANFAR will create some incremental increase to the base operations of that facility.   A 50~PB storage and compute facility operated under this model would have an annual operational cost in the neighbourhood of 11-14 M\$/year (7-11M\$/year incremental) and would support the vast majority of Canadian astronomy's computing and storage needs through to 2030 and beyond.  Such a facility would be built up over time and that  ramp-up is reflected in this costing estimate.

{\bf Systems and software} require continuous maintenance and updating to ensure they remain inter-operable between projects and facilities at the national and international level.  In the above storage/computing cost estimates, Canada benefits greatly from the ongoing operation of the CADC (approximately \$4M/year for 23 staff members) which provides broad support for operations, development and support of storage and computing infrastructure in astronomy.  Thousands of users and hundreds of research teams make use of the CADC/CANFAR system annually, with the CADC archive system providing access to data acquired by over 100 instruments via a single, uniform, interface.   

In addition to inter-operation software systems Canada must develop a focus in the area of supporting software tools.  These user tools, such as astropy and pyVO or DAOPHOT or TOPCAT require substantial and sustained efforts to ensure their stability and availability for research activities.

If astronomy chooses to not support storage and computing as a national resource, we will develop a fractured community.  Indeed, each piece of the community is likely too small to develop a significant role in astronomy data management and processing and Canada will quickly loose the capacity to develop and lead DRI developments in Astronomy.  The cost of not building a comprehensive DRI strategy are very high.

\end{lrptextbox}

\begin{lrptextbox}[What are the main programmatic risks
%Instructions: Programmatic risks include but are not limited to schedule, feasibility, budget, technical readiness level, computational or software requirements, dependence on other partners, and governance plan.
and how will they be mitigated?] 
%insert your text here
Formation of a new entity such as the CASC can become bogged down in territorial issues (geographic, research dicipline and wavelength based, to name a few).  CASC must develop as a truly nationally collaborative organization that supports the DRI needs of the entire community.  Establishing such an organization will not be easy and find appropriate and stable funding may not be possible.

The DRI support landscape is actively evolving and there is substantial risk that DRI will change in ways that are not suitable for the community.  We must engage, as a national community, with the `newORG' that is overseeing this evolution.  In addition, Canadian astronomers should build alliances with the other data-rich disciplines to help ensure that our needs our understood as being distinct from previously standard high performance computing infrastructure.  

Canadian astronomy must also develop a process to ensure that systems created to enhance cyber-infrastructure within research projects (such as CIRADA) can be integrated into the national infrastructure and with sustainable support.  A stronger connection between the operation and maintenance of CADC/CANFAR and the research community building these pieces is required to ensure that long term success of a Canadian Astronomy Science Centre.

\end{lrptextbox}

\begin{lrptextbox}[Does the proposed initiative offer specific tangible benefits to Canadians, including but not limited to interdisciplinary research, industry opportunities, HQP training,
%HQP=Highly qualified personnel, defined as individuals with university degrees at the bachelors' level and above.
EDI,
%EDI = equity, diversity and inclusion 
outreach or education?] 
%insert your text here
Since its inception in the 1980s, the CADC has worked to provide support for the development of HQP in digital infrastructure and `big-data' techniques. The third CADC staff member was a co-op student and the CADC continues to bring co-op and graduate students into the data centre to work on interesting and innovative problems.  A Canadian Astronomy Science Centre would fullfil and even larger role in the operation of DRI and would enhance astronomy's impact in the training of HQP in cyber/digital related areas.  Recently CADC staff have been working directly with graduate students (and some faculty) across Canada to enhance the communities knowledge and use of Machine Learning technologies.  CASC would provide a strong foundation to support strong collaborative interactions between the universities and NRC researchers and staff working in the digital field.
\end{lrptextbox}

\bibliography{main} 

\end{document}