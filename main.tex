% LRP2020 white paper template
% Search for "Instructions" below, and also see the call for white papers
% https://docs.google.com/document/d/1IT0g5AqaQM2FQQ0--M9qyQuWQ2906WlK_R-O32ZYoSY/
% Please don't change the page headings, margins or font size.
% HISTORY:
% 2019/06/27: v1.0 original version, v1.0
% 2019/07/12: v1.1 instructions added in executive summary section, re: cover page. 
% Changes to wording of text box questions 2, 6, 7. 
\documentclass[11pt]{article}
\usepackage{times}
\usepackage{geometry}
\geometry{letterpaper, portrait, margin=2cm}
\usepackage[utf8]{inputenc}
\usepackage{enumitem,amssymb}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{aas_macros}
\usepackage{mdframed} 
\usepackage{url}
\usepackage{hyperref}

\usepackage[authoryear]{natbib}
\bibliographystyle{apj}
\setcitestyle{authoryear,open={(},close={)}}

\mdfdefinestyle{theoremstyle}{
innertopmargin=\topskip,}
\mdtheorem[style=theoremstyle]{lrptextbox}{}

\pagestyle{fancy}
%Instructions:
%Please insert your expression of interest number of the form ENNN; see https://docs.google.com/spreadsheets/d/1_GqBxICZL0di_KvQoi_ZrdfNvqNnGotSYAoq0UqJYGc/ 
% and title (shorten if necessary) in the line below
\rhead{DRI-Astronomy}
\lhead{\thepage}
\renewcommand{\headrulewidth}{0pt}
\cfoot{}

% ****BEGIN EXECUTIVE SUMMARY SECTION****
% Instructions: A 5000-character-or-less executive summary will be requested on the white paper submission form.
%
% The white paper submission form will generate a cover page that will include the executive summary, topic area, author list and lead author contact information. Please do not include this information in the PDF generated with this template.


\begin{document}
% ****BEGIN MAIN WHITE PAPER SECTION****

% Instructions: Please insert your white paper text here.
%A white paper should be a self-contained description of a future opportunity for Canadian astronomy. A white paper will be most effective and useful if it concisely summarises and recommends an option that the LRP2020 panel should be considering for prioritisation.
%
% White papers are not required to contain a specific set of sections or headings. Depending on the content, the following topics may be appropriate to include:
%connection or relevance to Canada
%timeline 
%cost 
%description of risk
%governance / membership structure 
%justification for private submission of supplementary information
%This white paper will examine the expected evolution of digital research in support of astronomy research, focusing on the opportunity to create a digital research science platform for Canadian astronomy.
\setlength{\bibsep}{0.0pt}
\author{}
\title{Digital Research Infrastructure in Astronomy}
\maketitle
\section{Summary}

The Canadian astronomy community requires reliable and stable access to significantly resourced Digital Research Infrastructure (DRI) presented via science user focused interfaces.  The current funding model for both the hardware and software layers of the Canada's DRI is evolving and astronomers must take action to ensure that the DRI organization is aware of and able to address the communities needs.   Astronomy must have a coherent national voice through CASCA, but incorporating the expertise of CADC, CANFAR, and university science groups.

{\bf REC 1: Form a CASCA committee to develop strategies for developing a model for the effective utilization of NRC, CANARIE, CFI, Compute Canada, and other DRI funding sources for the benefit of Canadian astronomy. This committee should develop recommendations and report these to the CASCA Executive who can then engage with all of the involved funding partners to ensure a coherent model for the funding of DRI for astronomy.}

Canadian astronomy needs to increase the funding going into DRI and we must develop new community-wide structures to manage this infrastructure. The CADC is a central element of this infrastructure due to its ability to build a stable, world-leading base of expertise over 30 years backed by continuous NRC funding. 
Currently, funding of the hardware and software platform for research is being maintained via project focused CFI grants (such as CIRADA, CLASP, CHIME) within a funding model that excludes direct support for the CADC.  Separately, NRC has continued to support CADC's efforts to build the CANFAR science platforms capacity so that this can act as unifying system for the various individual projects.
Maintaining the CADC's world-leading expertise in the current system of funding via CFI grants whose funding distribution excludes CADC involvement is highly challenging.   Particularly challenging, from a data volume and computing needs, view will be supporting SKA, CHIME and possibly LSST data  within the increasingly complex data landscape. 
Ongoing and increased NRC funding to CADC is needed to deal with the challenges of creating an astronomy cyber-platform that allows the integration of Canada's astronomy data sets. 

{\bf CASCA should communicate this need to NRC and encourage increased support to CADC. CADC should develop a budget that is examined, critiqued, modified, and endorsed by CASCA in a communication to NRC.}

Where university based groups are able to create and manage some components of the necessary science infrastructure they should be encouraged to do so. The principal challenges for university groups are 1) to create infrastructure that serves a broad science community rather than a focussed research project, 2) integrating the infrastructure they create with existing national and international astronomy infrastructure and, most importantly, 3) ensuring the continued operation and ongoing development of their infrastructure reasonably far (more than a decade) into the future.

{\bf CASCA should communicate with these other bodies that provide funding and resources for astronomy and demonstrate a unified vision of the importance of digital infrastructure to astronomy. The complementary strengths of CADC and university groups should be emphasized.}

\section{Introduction}


As can be seen from even a cursory examination of the other LRP2020 white papers, Canadian astronomy is a data rich research endeavour.  Astronomical research has evolved to become a digital science, dependent on methods of analysis (W010, W004) and digital infrastructure (W011, W015, W026, CDC submission) and the collection of large survey data sets (W006, W018, W020, W025, W030, etc.).  The data related theme that runs through these white papers is that Canada has benefited strategically from the creation of the Canadian Astronomy Data Centre (CADC) and the communities ambitions reach well beyond the capacity of the current facility. 

To tackle the fundamental questions, astronomers are turning to ever larger data sets, made from surveying the sky with a variety of observational facilities operating across the energy spectrum. Building up a picture of the universe over a broad range of resolutions, timings and messengers.  At the same time, computing and mathematical methods are evolving towards techniques that are driven by AstroInformatics, Statistical Learning or Machine Learning (such as stellar classification and red-shift determination) and even evolving towards systems that use AI in their analysis  Within astronomy, we must pick up the pace in the deployment of these new technologies and in training the workforce.

In a modern astronomical research community Digital Research Infrastructure (DRI) should present to the community a domain specific view of the resource, providing access to software systems and tools that are designed to meet the research goals of that community.  Such a 'Science Portal' should be seen as facility in the same way that a telescope is a facility and the components of the systems are developed and replaced via instrumentation programs.  This is needed to help ensure that the astronomy community is getting highly effective access to DRI resources provided at the national level.

In the petabyte era the lines between software, technology and science are blurred - the chance to do science with petabytes without major infrastructure designed and operated to meet the need of the science user is pretty slim.  The importance of technology in science exploitation is becoming ever more important.

Formed in 1986 (yes, CADC will have its 40th anniversary during the implementation of LRP-2020) the CADC was envisioned to act as a centralized facility to provide access to both technical expertise in applications as well as physical capacity in data handling, processing. The founding data collection was seen as that coming from the Hubble Space Telescope but from the start the facility was expected to bring together data sets from across the observational spectrum to enable world leading astronomical research. Although located in a decentralized location (Victoria, BC) the then emerging network technologies made it clear that geography should not be a barrier to access.  However, the path between 1986 and today has not always been smooth.

At the time of writing, computing and storage infrastructure in Canada is operated by Compute Canada Inc. via support provided by federal and provincial governments but is in a state of substantial evolution.  For a discussion of this history and the coming changes please see the Computing and Data Committee submission to the LRP2020 panel.  
In brief, during the spring of 2019 the federal government of Canada initiated a process to restructure provisioning of computing for research through the establishment of a new entity that will operate the computing and storage systems in addition to supporting research software while the operation of research networking will remain with CANARIE.  
It is still early days in the establishing of the new 'DRI' operator and the community must pay attention to ensure that the details of the evolution meet our requirements.  The turmoil in the evolution of computing has been going on since the last CASCA LRP, suffice to say that there is some optimism that the new organization will be better funded (based on pledges made by the previous federal government) and run in a more research focused way (based on the draft structure that the new computing operator is expected to adopt). 

%\cite{2006JRASC.100....3S} Astronomy and Astrophysics are a data-rich research fields.   The world possesses a dizzying array of observational facilities (Alma, VLA, Subaru, Gemini, Apache Point Observatory, KPNO-4m, Hale, VLT1,2,3,4, SOAR, Magellan, Greenbank, DRAO, DAO, etc. etc.) and increasingly complex, more complete and data rich models (e.g. The Millennium Run, NuGrid, \textcolor{red}{OTHERS}), in addition to the existing (e.g. Gaia, SDSS, PanStarrs) and planned all-sky surveys (LSST, Euclid, WFIRST).   

\section{The diversity of DRI}
While considering the solutions to cyber-infrastructure in astronomy one must keep in mind the diverse scope of the discipline of Astronomy and Astrophysics. The community studies processes that span scales from the sub-atomic to, literally, the size of the universe.  
The various fields study processes that are driven by Newtonian physics, General Relativity, Fluid-dynamics, weak and strong forces and chemical and mechanical actions. 
This broad array of physical scales and mathematics requires a diverse cyber-infrastructure for support.
While some problems are well addressed by massively parallel {\bf supercomputer} systems using high-speed interconnected processors, other problems are better addressed through the use of dedicated GPU processing on a set of {\bf specialized hardware}.  
Still other problems are better better considered as {\bf high throughput computing} where massive amounts of data are processed in parallel via machines with some limited interconnected capacity and access to high-speed storage systems.  
Other problems are better suited to systems that straddle between interactive and high throughput computing making use of {\em virtualization and cloud infrastructure}.
When considering the solutions for cyber-infrastructure in astronomy it is important to keep in mind that this broad range of scales and capacities must all be satisfied for our diverse field to flurish and maintain its world leading impact.

\begin{itemize}
    \item[Compute:] this includes the capacity of computing needed to process raw data from a telescope into an science ready data product as well as the capacity needed to then turn that science product into a scientific insight.  Systems include the {\it super-computing} capacity needed to correlate radio signals from multi-antenna arrays, {\it high-through-put computing} needed to combine together imaging over multiple observing sessions, {\it specialized GPU and similar systems} needed to develop and train complex models, more and more frequently based on Machine Learning concepts and the {\it cloud infrastructure} (including the high-speed data layer) needed to allow direct interaction at scale.
    \item[Networks:] in-order to process the data from the array of facilities that Canadians have access to we must have the capacity to transfer those observations over the research internet.  Within the next decade the Vera Rubin Survey Telescope (LSST) will generate 2-3 petabytes of exportable data annually, requiring sustained bandwidth of 1 Gbit/s just to transfer a single copy as a continuous stream over one year and the Square Kilometre Array will generated multiple petabytes of data annually requiring networks capacity 10 to 100 times larger.  Big data projects, such as CHIME in particular, have been limited by the lack a robustly interconnected (CANARIE+NRC) research network.
    \item[Storage:] The CADC currently houses approximately 2-PB of astronomical observations, representing over 3 decades of archiving activities.  The LSST will produce this level of data annually!  The CADC is currently expanding its capacity to achieve 5 PB of usable storage but  this will only satisfy a few years of the data rates expected from facilities that are currently being archive (CFHT, JCMT, Gemini, TAOS-II, HST, JWST, DRAO, DAO).  There is currently no possibility of existing infrastructure supporting the next decades data behemoths.   Nationally, across all research disciplines, Compute Canada's system provide aggregated project storage of about 24PB, this is about the same size the would be required for the LSST-light data centre alone (W015).
    \item[Databases:] Here we refer to not the observational data files that are produced but to the catalogs of information derived from those observations.  Observational astrophysics, like most disciplines, is continuing to evolve into ever more specialized perspectives on problems. Roles in the observational problem often break-down into segments along a spectrum that ranges between the engineering and physics challenges of building ever more sensitive facilities to collect the signal (LIGO, IceCube, SNO, LHC, ALMA, Gemini, CFHT, JCMT, etc.) and their cohort of instruments and detectors that digitize that collected signal to the software systems that analyze that digitized signal to produce catalogs of measurements which can then be analyzed and correlated with signals from other facilities, each perhaps receiving different messengers of the same event. The measured signals from these facilities are being stored in ever larger databases of information. 
    
     In the era of LSST and SKA, the databases of measurements will grow in importance for science exploitation as handling the detector outputs becomes a more specialized activity.
    As of this writing (Fall 2019) the CDS-SIMBAD database in Strasbourg (which strives to aggregate the published astronomy information available for all identified objects and is updated on a daily basis) contains just over 35 million measurements of 10 million different objects.  While the CDS-Vizier system, which distributes tabular information accumulated in the literature, contains nearly 20,000 tables of data some (like the Gaia catalog) containing billions of entries.  Catalogs like Gaia and the coming LSST science catalog and then SKA measurement catalogs are produce as part of the facility operations and the science end user will achieve their science goals through the direct interaction of 'observing' the catalog data.  These catalogs will be multiple PetaBytes in size.  For even more sophisticated analysis the 'observer' will combine together information gleaned from examination of the cataloged measurements with access back to the detector outputs (spectra, pixels, time-series voltages), likely in processes driven by increasingly sophisticated machine learning approaches.  {\bf Canadian Astronomy has developed strong expertise in compute and storage components and has capacity in network infrastructure but is wearily lacking in database and catalog exploitation capacity. } 
 
    \item[Software:] From the purposes of this white paper we consider three broad categories: science analysis software, client application software for accessing infrastructure, digital services that expose the base infrastructure to the client.  Most instrumentation facilities have developed some level for each of these broad components, but few (if any) have substantively complete systems.  One of the great risks facing astronomy, and Canada in particular, in the digital era is that we  will not have the software systems needed to fully exploit the data sets we are creating.  At the infrastructure level the NRC, CANARIE and the CSA, through the operation of the CADC, the International Virtual Observatory Alliance (IVOA) and the CANFAR platform, are providing a strong base to build from.  Recently the CFI funded Canadian Initiative for Radio Astronomy Data Analysis (CIRADA) has begun to attempt to fill the gap in the science analysis component, but this gap is very large.  In particular the CIRADA project, by its very nature, is focused on the radio specific pieces of the problem.  In addition the focus of the efforts of CIRADA is on transforming calibrated observations into science ready data products, leaving the final hurdle of transforming those data products into science an unfunded activity.  The Canadian LSST Alert Science Platform (CLASP), a CFI proposal at this stage, is attempting to mirror the CIRADA project but for the LSST optical community.  What will become of the expertise developed through CIRADA and possibly CLASP when the projects end is unknown, {\bf a coherent centre for software systems must be advanced to address the broad range of software needs within the observational community.}
\end{itemize}

Canadian astronomy requires digital research infrastructure that can bring together these various digital pieces:  computing, storage,  networks, databases and software.  {\bf Given the expressed science need for data collections to cut across sub-domains (such as X-ray, optical, optical, infrared and radio astronomy) a single astronomy domain aware science portal that enables use of the full spectrum of this data is needed.}

Internationally a few platforms already exist (notably  \href{http://sciserver.org}{JHU's SciServer}, \href{https://datalab.noao.edu/}{NOAO DataLab} and \href{http://astrocloud.china-vo.org}{ChinaVO}) and many more are being planned or built, (e.g. \href{https://projectescape.eu/services/escape-science-platform}{Project Escape} and \href{https://docushare.lsst.org/docushare/dsweb/Get/LSE-319}{LSST Science Portal}).  
Interestingly, at this time, CANFAR is a more well resourced (from a computing hardware and storage view, accept compared to ChinaVO which, as of fall 2019, has over 2,000 active research users 10s of PB of storage and 10,000s of cores) than these other portals but the user layer of CANFAR requires substantial development to allow the same ease of use that is being achieved elsewhere.
These science portals are executing a vision of bringing astronomy research computing into a cloud based environment that crosses wavelength and research domains boundaries. 
The SciServe portal reaches even beyond the astronomy community, providing similar capabilities across the sciences and humanities research groups at JHU.
The concept of a science portal as the gateway to computing has become a ubiquitous modality. 

To be of utility these portals require stable long-term funding that is in addition to the base-infrastructure funding, without this stability researchers will become reliant on systems that churn in their behaviour and this will substantially impact productivity.  

A core component of some astronomers tool-boxes is the TopCat program that allows exploration and analysis of tabular data.  
TopCat was developed underfunding from 'starlink' to provide a user tool for interacting with large astronomical tables.
With the evolution of the funding model in the UK, starlink was disbanded and support of TopCat, that was being provided to the University of Bristol via starlink, was lost. 
The Table Access Protocol (TAP) is an IVOA standard for over-the-wire access to databases. 
TAP was developed by the IVOA to standardize access to databases over http(s).  
As TAP evolved (it is now nearly ubiquitous in astronomy data access) there was no capacity to support integration of new TAP components into TopCat and the utility of the underlying TopCat program was at risk of being lost if it could not be adapted to integrate these TAP services.
Fortunately, the power of TopCat was recognized and the Gaia project was able to support its continued maintenance and TopCat now has a robust TAP interface and is leading the development of connecting technologies for astronomy catalog-image interaction. 
This power tool allows users to access 100s of astronomical databases from a single application and cross match these catalogues for science analysis but was at risk of being lost due to a poorly matched funding model.

The University of Postdam provides access to many datasets via web interfaces that they have created (using there in-house built `daiquiri' framework).  Recent funding changes at Postdam mean that the lead developer of `daiquiri' is now `self-employed' by individual projects that need parts of the system updated and the main servers at Postdam are kept operational by a team of one person, who may soon move to other research activities.  

These projects are examples of high-value activities in the international astronomy community that are being run out of institutions that are no in a position to provide the long-term support that such services must have.  
Contrast these with services like the `Canadian Galactic Plane Survey' or the Herschel Archive which continue to operate and deliver high-value products after the end of mission.  Those are examples of services that are being provided by institutions with a mandate to maintain their long-term viability. 
Long-term stable funding is key for DRI to be successful.


\section{CADC, CVO and CANFAR}

Discussion of creation of the CADC dates from early 1980s with the official formation occurring around 1986.  The Winter Solstice 2986 edition of Cassiopeia contains Newsletter \#1 from the new Canadian Space Astronomy Data Centre, then led by Andy Woodsworth.  The initial mission of the CSADC was to:
\begin{itemize}
\item To obtain, catalog, and archive copies of all public domain data from HST (most data from the fiST become public domain one year after observation; the remainder become public domain immediately).
\item To make copies of requested portions of such data available to all Canadian scientists on request.
\item To operate a data reduction facility for the benefit of all Canadian astronomers .
\item To obtain and develop software suitable for the interpretation and reduction of HST
da.
\item  To assist and advise Canadian scientists in setting up their own data reduction facilities where appropriate.
\end{itemize}
and recognized that the data volumes and computing requires from HST would be complex for the majority of the community to properly handle.  This was in 1986, just as NRC was experiencing substantial funding cuts and HIA was being forced to take drastic steps to reduce their budget.  However, strong support from the astronomy community was found and CSADC was quickly changed to become the CADC.  While the community awaited the launch of HST the CADC took action to take advantage of the opportunities available 

{\it Since the proposal was developed, we have seen the potential for related services, such as providing access to a number of other astronomical catalogs and databases (e.g. IRAS, IUE) and facilities for analysis of large-format CCD images. We plan to offer these and other services as well as those strictly related to HST observations.}

Even from its very first inception, however, CADC struggled to achieve the budget capacity needed to achieve its mandated goals.  After significant interaction the Canadian Space Agency (CSA) entered into an MOU with NRC to co-fund the activities of the CADC. Under this MOU CSA provided, at the time, roughly 50\% of the support required to operate the CADC.   
At that initial inception CADC made technology choices that have proved highly successful and enabled the organization to be centre for astronomy archive innovation. This initial choice was to not mirror the systems and hardware operated by STScI for HST but instead to migrate information from that system to CADC's own systems. This allowed the CADC to pursue innovative search and retrieval options and we critical to the eventual development of 'on-the-fly' recalibration of HST data products and the production of deep stacks of HST imaging.  Importantly, at that point the CADC infrastructure used to support HST could also be used to support CFHT archiving which, initially, was the primary activity due to delays in HST launch and then problems with early operations. 

Fast forward to today and the highly insightful  original mission of the CADC is even more relevant today.  Although there have been ups and downs (including consideration of eliminating the CADC entirely) HAA-NRC continues to support the important mission of the CADC in partnership with the CSA. 
DRI for research is considerably more evolved today than in 1986 but the fundamental need for astronomy research to have a domain aware access layer remains. 

Starting in the late 2000 the CADC partnered with university researchers to form the Canadian Advanced Network For Astronomy research (CANFAR) to develop a science platform for astronomy layered atop Compute Canada's grid infrastructure.  This project was funded under CANARIE's Network Enable Platforms program.  Being an early implementation the CANFAR platform has undergone 3 technology transforms, each causing substantial modification of the computing interface.  These modification have required substantial effort to achieve.  Although existence for nearly a decade, the CANFAR requires substantial evolution to become a complete astronomy science platform. 

\subsection{International Virtual Observatory and Science Platforms}
As noted above there are numerous examples of Science Platforms being developed.  As the primary host of the Canadian Virtual Observatory the CADC has been participating in the IVOA efforts (from their outset) to ensure that data centres provide standardized interfaces that enable interoperability.  The success of this effort can be seen. for example, in the rich universe of TAP services that allow astronomers to probe an incredibly diverse sets of astronomically relevant catalogs of information. These efforts lead to what might be considered 'open data' in astronomical research. As we move into the Science Platform realm this concept of standardized service layers are even more critical as they provide the opportunity for platform interoperability, permitting scientists to have their research investigations to cross between centres.  As we work towards the concepts of 'code-to-data' (which is a fundamental driver of the science platform concept) we must keep in mind the interoperability of these science platforms our we will loose the achievement of open data by closing off the resources needed to access those data.

\section{Needed infrastrucutre}
The astronomy research community, along with other disciplines,  has a growing and strong need for increased storage and, concomitantly, compute capacity.
Examining the various white papers that have been submitted for LRP2020 we can see that over the next decade storage capacity must grow from the current scale of a few PetaBytes of online storage (enabling the storage of all Canadian astronomy data on live disks) to the capacity of 30-100s of PetaBytes of online storage and factors of many times more near-line capacity.  The CHIME project, for example, already generates PetaBytes per year of operation while LSST will generate tens of PetaBytes and SKA 100s.  
And astronomy is not alone, HEP, Gnomics and Climate (for example) are also demand increased capacity for large data.  We must, at this time of transition of Canada digital infrastructure management, clearly voice the needs of the research community.

Along with the predicted increase in storage data volumes will come an need to increase our computing capacity.  Within CANFAR, for example, the majority of computing is to 're-process' the observational datasets in novel ways so as to extract new information.  This, if we increase the stored data by a factor of 10-30 times we must also anticipate a need to increase our compute capacity in a similar way.  The need for compute is driven higher still by the development of new approaches to data analysis, in particular CNN based machine learning.  The CNN approach invariably requires access to GPU based computing to make the numerical computation needed to train deep networks possible. During the most recent round of computing allocations the demand for GPUs significantly out-stripped the available capacity  With growing data volumes and increased efficacy of techniques, the pressure on classical and GPU computing is likely to continue to grow.  Here too, the data-intensive research communities must organize themselves so that the correct models for delivery of computing infrastructure can be developed.


\section{Connection and relevance to Canada}
The CADC and CANFAR  provide the Canadian community with a strong advantage.
Canadian astronomy has a historic leadership role in the development of data standards relevant to astronomy via our participation in the IVOA.  We also have a long history of providing archive services for astronomical facilities and in connecting that archive storage to processing.  Within the CADC there is a partnership of actors who span the range of activities (from operations to development through to science use and public dissemination).  The historic background and staffing design has made the CADC facility an important asset for the Canadian community, one that can, and has been, leveraged to provided access to other resources and facilities. 

In the coming decade Canadian astronomy has ambitions to participate in a growing array of data rich experiments:  Euclid, LSST, SKA, WFIRST.  These projects will require the capacity for storage and processing to grow from the current level of 2-3 PB of active storage to 50-100 PB of active capacity.  There does not exist, today, capacity within  Compute Canada's community storage system sufficient capacity to enable the astronomy ambitions.     Along with this storage will come a need for processing.  This need will, roughly, scale with the annual growth in storage.   Given the current installation of 5-PB requires approximately 1500 CPU cores of processing for effective use, one can estimate that a 50-PB storage system will require 15,000 CPU cores.  

In addition, software systems capable of managing the storage and processing system must be developed and enhanced.  Within the SKA project there are plans to adapt CERN-LHC storage systems like EOS and RUCIO for use in astronomy.  These software systems have a strong pedigree but have been designed to satisfy the particle physics communities needs and modes of operations.  Astronomy will need to invest significant effort to ensure software technologies exists to meet this communities needs.  These technologies should support IVOA developed standards and it is conceivable that the storage systems themselves could emerge as a community standard. 

As Canada seeks to grown our digital economy, astronomy data storage and processing needs could be used as a catalyst for enhancing our national digital capacity.

\section{Timeline}
The major data missions will require storage and processing capacity to rapidly grow, beginning in the early 2020s.   By the end of LRP2020 Canadian astronomy will require a fully operational storage and processing capacity capable of handling 50 PB of astronomical data. This is about ten times the capacity that is currently (Fall 2019) being deployed for astronomy research needs.

\section{Cost}
Storage and processing costs have been, and continue to, trend lower as technology enables densification of capacity.   Within Canada the costing estimates determined for a Canadian SKA Science Centre are the highest fidelity estimates available.  There we have estimated the cost of running a 23PB storage system at roughly \$4.5M annually (includes all staffing [including some science user support], storage and processing costs), giving a costing of \$200K/PB/year of storage and its concomitant processing needs. These are the current day cost, there is an expectation that this number would scale lower in to 2030s and beyond, but it is likely that the communities storage needs would grow and it is conceivable that this annual budget spend might be someone flat.

\section{Description of risk}
There is not capacity with the current Compute Canada general storage and compute system to support the desired level of storage and processing.  In addition, the model of operations of Compute Canada does not fully support the astronomy research communities needs (the same is also true of other data rich research communities, such as HEP).   As the newOrg structure for Digital Research Infrastructure is developed this organization may make things better or worse, Canadian astronomy must strongly engage with this newOrg to ensure that the our needs will be properly addressed.  

In addition to the hardware systems and their architecture, the community also requires a science platform to make use of those hardware systems transparent and efficient.   This will requiring build a strong domain specific science platform.  The world astronomy community is already engaging in the building of such systems and Canada must continue to strongly engage in this effort.  In the current funding model,  university level groups are funded to develop specific pieces relevant to their research needs.  However, we do not want that effort to be siloed within the specific research problem and those research infrastructure pieces must be supported beyond the life of the project that created them, or the effort will be wasted.

 
\section{Governance/membership structure}




\begin{lrptextbox}[How does the proposed initiative result in fundamental or transformational advances in our understanding of the Universe?]
Individual telescope and research projects currently must build their own access layer to enable astronomy research on Compute Canada Federation (CCF) infrastructure.  An enhanced CANFAR will remove this burden from the individual projects and provide enhanced capacity to users, reducing the time lag between data acquisition and science (ie. increase productivity).
\end{lrptextbox}

\begin{lrptextbox}[What are the main scientific risks and how will they be mitigated?]
Main risk will be that the science platform infrastructure will not meet the needs of the community its meant to serve.  This risk will be mitigated by establishing a governance and advisory structure that gives the stakeholder community control of the resource development.  This body will run a Science Advisory group that will review the development choices of the platform to ensure they are working towards meeting the broad goals of the community.  
\end{lrptextbox}

\begin{lrptextbox}[Is there the expectation of and capacity for Canadian scientific, technical or strategic leadership?] 
The Canadian Astronomy Data Centre has developed strong expertise in the field of  astronomy data management. This expertise is evidenced by CADC's leadership role of the development of data standards within the International Virtual Observatory.  The CANFAR science portal as it stands today provides some of the service capabilities that are needs by an astronomy science platform and has been developed by the CADC in collaboration with CCF and the university research community. 
\end{lrptextbox}

\begin{lrptextbox}[Is there support from, involvement from, and coordination within the relevant Canadian community and more broadly?] 
CANFAR has some of the required structure to allow engagement with the national community.  Recently, however, a number of  specific and separate portals within the astronomy community have been or are being developed. 
CANFAR, through the CADC has been working with specific 
\end{lrptextbox}


\begin{lrptextbox}[Will this program position Canadian astronomy for future opportunities and returns in 2020-2030 or beyond 2030?] 
Building out of a science platform along with the storage and computing capacity required for the various projects planned in the 2020s will help build the capacity required for Canadian astronomers to continue to be participants in modern astronomy.  The big-data projects like LSST, SKA and Euclid will begin operation in the 2020s and continue will into the 2030s.

\end{lrptextbox}

\begin{lrptextbox}[In what ways is the cost-benefit ratio, including existing investments and future operating costs, favourable?] 

The cost of computing and storage roughly scale together.   Averaged over expected pricing between 2020 and 2030 the full costs of operations is about \$200K/PB/Year (assuming one is running at 20 PB storage facility,  development and support staffing and some other costs do not scale directly with storage/compute levels). This costing is similar to the cost of single full-time staff position at a Canadian research university.  In 2018 4,606 individual computers and 248 authenticated users (most data transfers are for public data and are anonymous) connected to the CANFAR/CADC storage system and retrieved about 1PB of data from that system.  The CADC/CANFAR  storage system currently has a capacity of about 2PB.  The hardware costs of supporting those 248 users has a cost similar to that of funding a research faculty position.  

In these costing estimates, Canada benefits greatly from the ongoing operation of the CADC (approximately \$4M/year for 23 staff members) which provides broad support for operations, development and support of storage and computing infrastructure in astronomy.   Expanding the storage and computing capacity of the systems developed and maintained by the CADC within CANFAR will create some incremental increase to the base operations of that facility.   A 50PB storage and compute facility operated under this model would have an annual operational cost in the neighbourhood of $14M/year ($10M/year incremental) and would support the vast majority of Canadian astronomy computing and storage needs through to 2030 and beyond.  

If Canada chooses to not support storage and computing as a national resource we will develop a fractured community.  Indeed, each piece of the community is likely too small to develop a significant role in astronomy data management and processes and Canada will quickly loose the capacity to develop and lead DRI developments in Astronomy.  The cost of not building a large national DRI are very high.

\end{lrptextbox}

\begin{lrptextbox}[What are the main programmatic risks
%Instructions: Programmatic risks include but are not limited to schedule, feasibility, budget, technical readiness level, computational or software requirements, dependence on other partners, and governance plan.
and how will they be mitigated?] 
%insert your text here
The DRI support landscape is actively evolving and there is substantial risk that DRI will change in ways that are not suitable for the community.  We must engage, as a national community, with the 'newORG' that is over seeing this evolution.  In addition, Canadian astronomers should build alliances with the other data-rich disciplines to help ensure that our needs our understood as being distinct from previously standard HPC infrastructure.  

Canadian astronomy must also develop a process to ensure that systems developed to enhance cyber-infrastructure within research projects (such as CIRADA) can be well integrated into the national infrastructure and sustainably supported.  A stronger connection between the operation and maintenance of CADC/CANFAR and the research community building this pieces is required to ensure that long term success of a Canadian Astronomy Science Centre.

\end{lrptextbox}

\begin{lrptextbox}[Does the proposed initiative offer specific tangible benefits to Canadians, including but not limited to interdisciplinary research, industry opportunities, HQP training,
%HQP=Highly qualified personnel, defined as individuals with university degrees at the bachelors' level and above.
EDI,
%EDI = equity, diversity and inclusion 
outreach or education?] 
%insert your text here

\end{lrptextbox}

\bibliography{example} 

\end{document}